{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy Example above TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn! The rest of the notebook takes you through the process of creating a classifier on your own. An example implementation is already filled in to help you get started. \n",
    "\n",
    "The exercises are there to guide you, but feel free to experiment beyond them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "For this task we will using a real movie review dataset. The dataset contains texts of reviews and a sentiment label (0 = bad, 1=good). Both the train and test sets contain 50\\% of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I felt a great joy, after seeing this film, no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  I felt a great joy, after seeing this film, no...          1"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.data')\n",
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I felt a great joy, after seeing this film, not because it is a master piece, but because it convinced me of, that the Portuguese cinema became really very good. We can see here the best Portuguese actores in this field.\n"
     ]
    }
   ],
   "source": [
    "print(train_data.text.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henry Thomas showed a restraint, even when the third act turned into horrible hollywood resolution that could've killed this movie, that kept the dignity of a redemption story and as for pure creepiness-sniffing babies?\n"
     ]
    }
   ],
   "source": [
    "print(train_data.text.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film is pretty good, it actually is like ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  This film is pretty good, it actually is like ...          1"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.data')\n",
    "test_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in a Pandas DataFrame. \n",
    "\n",
    "To access the columns use `train_data.text` or `train_data.sentiment`.\n",
    "\n",
    "If you are having trouble with it, I've put each column into a separate variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data.text.values\n",
    "train_sentiment = train_data.sentiment.values\n",
    "\n",
    "test_test = test_data.text.values\n",
    "test_sentiment = test_data.sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the features\n",
    "\n",
    "The next step is to create features. Scikit-learn has a couple of different functions to help us:\n",
    "\n",
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): Counts the words in a piece of text.\n",
    "\n",
    "[TfidfVectorier](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer): Scaled version of counts - weighs rare words higher\n",
    "\n",
    "Note: n-grams refer to using phrases of length n (e.g 'very cool' is a bigram/2-gram)\n",
    "\n",
    "*Exercise: Try using binary counts instead of actual counts*\n",
    "\n",
    "*Exercise: Try using the TfidfVectorizer and compare performance*\n",
    "\n",
    "*Exercise: Try different settings for ngram_range - which range is optimal?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# first call fit - to know which words you are counting\n",
    "vec.fit(train_data.text)\n",
    "\n",
    "# Now create a matrix from the words - the Xs refer to the matrix form\n",
    "X_train = vec.transform(train_data.text)\n",
    "X_test = vec.transform(test_data.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model\n",
    "\n",
    "- [K Nearest Neighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)\n",
    "- [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
    "- [RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "For each classifier, the workflow is similar:\n",
    "\n",
    "1. Create classifier and specify parameters\n",
    "2. Call the `fit` method on the training data\n",
    "3. Call score to get accuracy on the test data\n",
    "\n",
    "\n",
    "**Exercise: Try each of the classifiers above and compare the performance**\n",
    "\n",
    "**Exercise: Try changing the \"K\" in the KNN method**\n",
    "\n",
    "**Exercise: Try changing the number of trees (n_estimators) in the Random Forest Method**\n",
    "\n",
    "**Exercise: Try changing the other parameters under Random Forest Method**\n",
    "\n",
    "Feel free to experiment with the settings in the other classifiers, but the Random Forest ones are the most intuitive. \n",
    "\n",
    "\n",
    "Each classifier comes with a number of parameters. In practice, these parameters are not set by hand, because it can be hard predict the effects of each one. Instead, the best parameter settings are found automatically through a process called *hyperparameter tuning*. You can read about it [here](http://scikit-learn.org/0.15/auto_examples/randomized_search.html), but it basically means using a script to try a bunch of parameter settings.\n",
    "\n",
    "*Bonus Exercise: Experiment with [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) or with the [Bagging Classifier](http://scikit-learn.org/0.15/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)*\n",
    "\n",
    "*Bonus Exercise: Create an [ensemble](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) of your favorite classifiers. (Hint: you pass in the classifiers you want to use)*\n",
    "\n",
    "*Bonus Exercise: Use the link above to automatically try different settings for one of the classifiers*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastassia.kornilova/miniconda3/envs/ml_demo/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=None,\n",
       "      validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Perceptron is a very basic linear model\n",
    "clf = Perceptron()\n",
    "\n",
    "# Specify input than output\n",
    "clf.fit(X_train, train_data.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the accuracy. \n",
    "\n",
    "Use the exercises above to see how different changes affect the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.758\n"
     ]
    }
   ],
   "source": [
    "# Score method outputs the accuracy\n",
    "print(\"Accuracy:\", clf.score(X_test, test_data.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
